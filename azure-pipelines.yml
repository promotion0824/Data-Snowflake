# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- main

parameters:
- name: keyVaultSubscriptionName
  type: string
  default: az-data-prd-Data
 
- name: keyVaultName
  type: string
  default: 'wil-dsecore-prd-kv-eu22'

# using the same name here since I no longer have access to this vault.
- name: degreeDaysStorageAccount
  type: string
  default: 'wilsfstgwilaueprddlsaue1'

steps:
- checkout: none

- task: AzureCLI@2
  displayName: Get agent IP address
  inputs:
    azureSubscription: ${{ parameters.keyVaultSubscriptionName }}
    scriptType: 'bash'
    scriptLocation: 'inlineScript'
    inlineScript: |
      ip=$(curl -s "http://ipinfo.io/json" | jq '.ip')
      echo "Agent IP address: ${ip}"
      echo "##vso[task.setvariable variable=agentIpAddress]$ip"

- task: AzureCLI@2
  displayName: Create key vault firewall rule for the agent
  inputs:
    azureSubscription: '${{ parameters.keyVaultSubscriptionName }}'
    scriptType: 'bash'
    scriptLocation: 'inlineScript'
    inlineScript: |
      echo 'Agent IP Address: $(agentIpAddress) '
      az keyvault network-rule add  --name ${{ parameters.keyVaultName }} --ip-address $(agentIpAddress) 
      az storage account network-rule add --account-name ${{ parameters.degreeDaysStorageAccount }} --ip-address $(agentIpAddress)

- task: AzureCLI@2
  displayName: Sleep for 45 seconds
  inputs:
    azureSubscription: '${{ parameters.keyVaultSubscriptionName }}'
    scriptType: 'bash'
    scriptLocation: 'inlineScript'
    inlineScript: sleep 45

- task: AzureKeyVault@1
  displayName: Download key vault secrets
  inputs:
    azureSubscription: '${{ parameters.keyVaultSubscriptionName }}'
    KeyVaultName: '${{ parameters.keyVaultName }}'
    SecretsFilter: 'DegreeDays-StorageAccount-SasToken'
    RunAsPreJob: false   

- task: Bash@3
  displayName: 'Installing libraries'
  inputs:
    targetType: 'inline'
    script: |
      pip install GitPython
      pip install typing-extensions
      pip install azure.identity
      pip install azure.keyvault.secrets
      pip install azure.storage.blob
- task: PythonScript@0
  displayName: Getting Ontology

  inputs:
    scriptSource: 'inline'
    script: |
      from git import Repo
      Repo.clone_from('https://github.com/WillowInc/opendigitaltwins-building', '/home/vsts/work/Ontology')

- task: Bash@3
  displayName: 'listing Ontology folders . .  . '
  inputs:
    targetType: 'inline'
    script: |
      pwd
      cd /home/vsts/work/Ontology
      cd Ontology/Willow
      ls

- task: PythonScript@0
  displayName: Transforming Ontology
  inputs:
    arguments: $(DegreeDays-StorageAccount-SasToken) ${{ parameters.degreeDaysStorageAccount }} $(Build.SourcesDirectory)
    scriptSource: 'inline'
    script: |
      from git import Repo
      import os
      import json
      import csv
      import codecs
      from typing_extensions import Concatenate 
      import sys
      from io import StringIO 
      import gzip
      import time
      from azure.identity import DefaultAzureCredential
      from azure.keyvault.secrets import SecretClient
      from azure.storage.blob import *

      sas_token = sys.argv[1]
      storage_account = sys.argv[2]
      local_path = sys.argv[3]

      accountEndpoint = "https://" + (storage_account) + ".blob.core.windows.net"
      sasToken = sas_token
      containerName = 'wilaue-wo77920-adhoc-stage'
      ontology_blob = 'ontology/ontology_buildings.csv'

      directory = '/home/vsts/work/Ontology/Ontology/Willow/'
      header = ['path', 'key_value']
      data_list = []
      output_file = '/home/vsts/work/Ontology/ontology_buildings.csv'
      with open(output_file, 'w', newline='') as outfile:
        writer = csv.writer(outfile)
        writer.writerow(header)
      for root, subdirectories, files in os.walk(directory):
          for subdirectory in subdirectories:
              foldername = os.path.join(root, subdirectory)
          for file in files:
            filename = os.path.join(root, file)
            print(filename)
            if filename.endswith(".json"):
                  with open(filename, 'r',encoding='utf-8-sig') as f:
                      with open(output_file, 'a', newline='') as outfile:
                          
                          loaded_json = json.load(f)

                          filepath = filename.replace(directory,'Willow')
                          paths = filepath.split('\\')
                          data =(paths, loaded_json)
                          writer = csv.writer(outfile)
                          writer.writerow(data)
      outfile.close()
      blob_service_client = BlobServiceClient(account_url=accountEndpoint, credential=sasToken)

      blob_client = blob_service_client.get_blob_client(container=containerName, blob=ontology_blob)
      with open(output_file, "rb") as data:
        blob_client.upload_blob(data, blob_type="BlockBlob",overwrite=True)